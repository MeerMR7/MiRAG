import streamlit as st
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

#UI
st.set_page_config(page_title="MiRAG | PDF Chat", page_icon="ðŸ”®")

st.markdown("""
Â  Â  <style>
Â  Â  .developer-tag { text-align: right; color: #6c757d; font-size: 14px; margin-top: -20px; }
Â  Â  </style>
Â  Â  """, unsafe_allow_html=True)

st.title("ðŸ”® MiRAG")
st.markdown("<div class='developer-tag'>Developed By HasMir</div>", unsafe_allow_html=True)
st.markdown("---")

#SIDEBAR
with st.sidebar:
Â  Â  st.header("Settings")
Â  Â  api_key = st.text_input("Enter OpenAI API Key", type="password")
Â  Â Â 
Â  Â  st.subheader("Your Documents")
Â  Â  uploaded_pdfs = st.file_uploader("Upload PDF files", type="pdf", accept_multiple_files=True)
Â  Â Â 
Â  Â  if st.button("Clear Chat"):
Â  Â  Â  Â  st.session_state.messages = []
Â  Â  Â  Â  st.rerun()

#RAG ENGINE
def create_knowledge_base(pdfs, key):
Â  Â  all_docs = []
Â  Â  for pdf in pdfs:
Â  Â  Â  Â  # Temporary save to allow PyPDFLoader to read it
Â  Â  Â  Â  with open("temp.pdf", "wb") as f:
Â  Â  Â  Â  Â  Â  f.write(pdf.getbuffer())
Â  Â  Â  Â  loader = PyPDFLoader("temp.pdf")
Â  Â  Â  Â  all_docs.extend(loader.load())
Â  Â  Â  Â  os.remove("temp.pdf") # Clean up

Â  Â  # Split: Break PDF text into 1000-character chunks
Â  Â  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
Â  Â  final_chunks = text_splitter.split_documents(all_docs)

Â  Â  # Embed & Store: Convert text to math (vectors) and save in FAISS
Â  Â  embeddings = OpenAIEmbeddings(openai_api_key=key)
Â  Â  vectorstore = FAISS.from_documents(final_chunks, embeddings)
Â  Â  return vectorstore.as_retriever()

#CHAT LOGIC
if "messages" not in st.session_state:
Â  Â  st.session_state.messages = []

# Display history
for msg in st.session_state.messages:
Â  Â  st.chat_message(msg["role"]).write(msg["content"])

# User Input
if prompt := st.chat_input("Ask a question about your PDFs"):
Â  Â  if not api_key:
Â  Â  Â  Â  st.error("Please provide an API Key in the sidebar.")
Â  Â  else:
Â  Â  Â  Â  st.session_state.messages.append({"role": "user", "content": prompt})
Â  Â  Â  Â  st.chat_message("user").write(prompt)

Â  Â  Â  Â  with st.chat_message("assistant"):
Â  Â  Â  Â  Â  Â  with st.spinner("MiRAG is scanning documents..."):
Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Initialize Brain
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  llm = ChatOpenAI(model="gpt-4o-mini", openai_api_key=api_key)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if uploaded_pdfs:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # RAG Pipeline
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  retriever = create_knowledge_base(uploaded_pdfs, api_key)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  system_prompt = (
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "You are MiRAG, a precise document assistant. "
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Use the context below to answer. If the answer isn't in the context, "
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "say you don't know based on the files provided.\n\n{context}"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prompt_template = ChatPromptTemplate.from_messages([
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ("system", system_prompt),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ("human", "{input}"),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  combine_docs_chain = create_stuff_documents_chain(llm, prompt_template)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rag_chain = create_retrieval_chain(retriever, combine_docs_chain)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response = rag_chain.invoke({"input": prompt})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  full_res = response["answer"]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Fallback to general AI if no PDF
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  full_res = llm.invoke(prompt).content

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(full_res)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.messages.append({"role": "assistant", "content": full_res})
Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Error: {str(e)}")
